{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***负面评论识别：先不用word2vec，及doc2vec提取特征\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_time: 2021-11-09 18:43:00\n",
      "WARNING:tensorflow:From D:\\anaconda3_5\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3_5\\lib\\site-packages\\dask\\dataframe\\utils.py:14: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end_time: 2021-11-09 18:43:33\n"
     ]
    }
   ],
   "source": [
    "#导入相关的包\n",
    "import datetime\n",
    "print('start_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import os\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_1d, global_max_pool\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.merge_ops import merge\n",
    "from tflearn.layers.estimator import regression\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from tflearn.layers.normalization import local_response_normalization\n",
    "from tensorflow.contrib import learn\n",
    "\n",
    "#给定特征的最大个数,以及文档的长度\n",
    "\"\"\"\n",
    "把下面这两个参数调小一点，便于在自己的电脑上运行\n",
    "\"\"\"\n",
    "max_features=200\n",
    "max_document_length=500  #词汇表模型中需要用到,这个参数的取值会影响到cnn模型的正常运行(out of list)\n",
    "vocabulary=None\n",
    "\n",
    "print('end_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_time: 2021-11-09 18:43:33\n",
      "end_time: 2021-11-09 18:43:33\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "读取文件数据,看起来像是将一个文档(文件)加载到一个字符串,然后再保存到列表中。\n",
    "\"\"\"\n",
    "print('start_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "def load_one_file(filename):\n",
    "    x=\"\"\n",
    "    with open(filename,encoding='gb18030', errors='ignore') as f:\n",
    "        for line in f:\n",
    "            line=line.strip('\\n')\n",
    "            line = line.strip('\\r')\n",
    "            x+=line\n",
    "    f.close()\n",
    "    return x\n",
    "\n",
    "def load_files_from_dir(rootdir):\n",
    "    x=[]\n",
    "    list = os.listdir(rootdir)\n",
    "    for i in range(0, len(list)):\n",
    "        path = os.path.join(rootdir, list[i])\n",
    "        if os.path.isfile(path):\n",
    "            v=load_one_file(path)\n",
    "            x.append(v)\n",
    "    return x\n",
    "\n",
    "def load_all_files():\n",
    "    x_train=[]\n",
    "    y_train=[]\n",
    "    x_test=[]\n",
    "    y_test=[]\n",
    "    path=\"E:/pycharm_project/deep_learning_web_safe/review/data/train/pos/\"\n",
    "    print (\"Load %s\" % path)\n",
    "    x_train=load_files_from_dir(path)\n",
    "    y_train=[0]*len(x_train)\n",
    "    path=\"E:/pycharm_project/deep_learning_web_safe/review/data/train/neg/\"\n",
    "    print (\"Load %s\" % path)\n",
    "    tmp=load_files_from_dir(path)\n",
    "    y_train+=[1]*len(tmp)\n",
    "    x_train+=tmp\n",
    "\n",
    "    path=\"E:/pycharm_project/deep_learning_web_safe/review/data/test/pos/\"\n",
    "    print (\"Load %s\" % path)\n",
    "    x_test=load_files_from_dir(path)\n",
    "    y_test=[0]*len(x_test)\n",
    "    path=\"E:/pycharm_project/deep_learning_web_safe/review/data/test/neg/\"\n",
    "    print (\"Load %s\" % path)\n",
    "    tmp=load_files_from_dir(path)\n",
    "    y_test+=[1]*len(tmp)\n",
    "    x_test+=tmp\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "print('end_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/train/pos/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/train/neg/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/test/pos/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/test/neg/\n",
      "CountVectorizer(decode_error='ignore', max_features=200, stop_words='english',\n",
      "                strip_accents='ascii')\n",
      "CountVectorizer(decode_error='ignore', stop_words='english',\n",
      "                strip_accents='ascii',\n",
      "                vocabulary={'10': 0, 'acting': 1, 'action': 2, 'actor': 3,\n",
      "                            'actors': 4, 'actually': 5, 'american': 6,\n",
      "                            'audience': 7, 'away': 8, 'awful': 9, 'bad': 10,\n",
      "                            'beautiful': 11, 'believe': 12, 'best': 13,\n",
      "                            'better': 14, 'big': 15, 'bit': 16, 'black': 17,\n",
      "                            'book': 18, 'boring': 19, 'br': 20, 'budget': 21,\n",
      "                            'camera': 22, 'cast': 23, 'character': 24,\n",
      "                            'characters': 25, 'classic': 26, 'come': 27,\n",
      "                            'comedy': 28, 'comes': 29, ...})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25000, 200)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "一:特征提取：1，词袋模型\n",
    "\"\"\"\n",
    "print('start_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "def get_features_by_wordbag():\n",
    "    global max_features\n",
    "    x_train, x_test, y_train, y_test=load_all_files()\n",
    "\n",
    "    vectorizer = CountVectorizer(\n",
    "                                 decode_error='ignore',\n",
    "                                 strip_accents='ascii',\n",
    "                                 max_features=max_features,\n",
    "                                 stop_words='english',\n",
    "                                 max_df=1.0,\n",
    "                                 min_df=1 )\n",
    "    print (vectorizer)\n",
    "    x_train=vectorizer.fit_transform(x_train)\n",
    "    x_train=x_train.toarray()\n",
    "    vocabulary=vectorizer.vocabulary_\n",
    "    \n",
    "    #用训练集生成的词袋模型对测试数据进行词袋化处理\n",
    "    vectorizer = CountVectorizer(\n",
    "                                 decode_error='ignore',\n",
    "                                 strip_accents='ascii',\n",
    "                                 vocabulary=vocabulary,\n",
    "                                 stop_words='english',\n",
    "                                 max_df=1.0,\n",
    "                                 min_df=1 )\n",
    "    print (vectorizer)\n",
    "    x_test=vectorizer.fit_transform(x_test)\n",
    "    x_test=x_test.toarray()\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "x_train, x_test, y_train, y_test = get_features_by_wordbag()\n",
    "x_train\n",
    "x_train.shape\n",
    "#(25000,300)\n",
    "\n",
    "print('end_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_time: 2021-11-07 15:00:23\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/train/pos/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/train/neg/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/test/pos/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/test/neg/\n",
      "CountVectorizer(binary=True, decode_error='ignore', max_features=200,\n",
      "                stop_words='english', strip_accents='ascii')\n",
      "CountVectorizer(binary=True, decode_error='ignore', stop_words='english',\n",
      "                strip_accents='ascii',\n",
      "                vocabulary={'10': 0, 'acting': 1, 'action': 2, 'actor': 3,\n",
      "                            'actors': 4, 'actually': 5, 'american': 6,\n",
      "                            'audience': 7, 'away': 8, 'bad': 9, 'beautiful': 10,\n",
      "                            'believe': 11, 'best': 12, 'better': 13, 'big': 14,\n",
      "                            'bit': 15, 'black': 16, 'boring': 17, 'br': 18,\n",
      "                            'budget': 19, 'came': 20, 'camera': 21, 'cast': 22,\n",
      "                            'character': 23, 'characters': 24, 'classic': 25,\n",
      "                            'come': 26, 'comedy': 27, 'comes': 28,\n",
      "                            'completely': 29, ...})\n",
      "end_time: 2021-11-07 15:03:20\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "一:特征提取：2，TF-IDF模型\n",
    "\"\"\"\n",
    "print('start_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "def get_features_by_wordbag_tfidf():\n",
    "    global max_features\n",
    "    x_train, x_test, y_train, y_test=load_all_files()\n",
    "\n",
    "    vectorizer = CountVectorizer(\n",
    "                                 decode_error='ignore',\n",
    "                                 strip_accents='ascii',\n",
    "                                 max_features=max_features,\n",
    "                                 stop_words='english',\n",
    "                                 max_df=1.0,\n",
    "                                 min_df=1,\n",
    "                                 binary=True)\n",
    "    print (vectorizer)\n",
    "    x_train=vectorizer.fit_transform(x_train)\n",
    "    x_train=x_train.toarray()\n",
    "    vocabulary=vectorizer.vocabulary_\n",
    "   #用训练集生成的词袋模型对测试数据进行词袋化处理\n",
    "    vectorizer = CountVectorizer(\n",
    "                                 decode_error='ignore',\n",
    "                                 strip_accents='ascii',\n",
    "                                 vocabulary=vocabulary,\n",
    "                                 stop_words='english',\n",
    "                                 max_df=1.0,binary=True,\n",
    "                                 min_df=1 )\n",
    "    print (vectorizer)\n",
    "    x_test=vectorizer.fit_transform(x_test)\n",
    "    x_test=x_test.toarray()\n",
    "\n",
    "    transformer = TfidfTransformer(smooth_idf=False)\n",
    "    x_train=transformer.fit_transform(x_train)\n",
    "    x_train=x_train.toarray()\n",
    "    x_test=transformer.transform(x_test)\n",
    "    x_test=x_test.toarray()\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "x_train, x_test, y_train, y_test = get_features_by_wordbag_tfidf()\n",
    "x_train\n",
    "x_train.shape\n",
    "\n",
    "print('end_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_time: 2021-11-09 00:08:11\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/train/pos/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/train/neg/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/test/pos/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/test/neg/\n",
      "WARNING:tensorflow:From D:\\anaconda3_5\\lib\\site-packages\\tflearn\\data_utils.py:211: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From D:\\anaconda3_5\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\preprocessing\\text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From D:\\anaconda3_5\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\preprocessing\\text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "end_time: 2021-11-09 00:10:56\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "一:特征提取：3，词汇表模型，\n",
    "\"\"\"\n",
    "print('start_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "def  get_features_by_tf():\n",
    "    global  max_document_length\n",
    "    x_train, x_test, y_train, y_test=load_all_files()\n",
    "\n",
    "    vp=tflearn.data_utils.VocabularyProcessor(max_document_length=max_document_length,\n",
    "                                              min_frequency=0,\n",
    "                                              vocabulary=None,\n",
    "                                              tokenizer_fn=None)\n",
    "    x_train=vp.fit_transform(x_train, unused_y=None)\n",
    "    x_train=np.array(list(x_train))\n",
    "\n",
    "    x_test=vp.transform(x_test)\n",
    "    x_test=np.array(list(x_test))\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "x_train, x_test, y_train, y_test = get_features_by_tf()\n",
    "x_train\n",
    "x_train.shape\n",
    "\n",
    "print('end_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_time: 2021-11-09 18:57:50\n",
      "end_time: 2021-11-09 18:57:50\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "二：定义训练模型:\n",
    "\"\"\"\n",
    "print('start_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))\n",
    "#通过贝叶斯模型的效果来对特征的个数做决定。\n",
    "def show_diffrent_max_features():\n",
    "    global max_features\n",
    "    a=[]\n",
    "    b=[]\n",
    "    for i in range(1000,20000,2000):\n",
    "        max_features=i\n",
    "        print (\"max_features=%d\" % i)\n",
    "        #x, y = get_features_by_wordbag()\n",
    "        #x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.4, random_state=0)\n",
    "        x_train, x_test, y_train, y_test=get_features_by_wordbag()\n",
    "        gnb = GaussianNB()\n",
    "        gnb.fit(x_train, y_train)\n",
    "        y_pred = gnb.predict(x_test)\n",
    "        score=metrics.accuracy_score(y_test, y_pred)\n",
    "        a.append(max_features)\n",
    "        b.append(score)\n",
    "        plt.plot(a, b, 'r')\n",
    "    plt.xlabel(\"max_features\")\n",
    "    plt.ylabel(\"metrics.accuracy_score\")\n",
    "    plt.title(\"metrics.accuracy_score VS max_features\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "#贝叶斯模型\n",
    "def do_nb_wordbag(x_train, x_test, y_train, y_test):\n",
    "    #print (\"NB and wordbag\")\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(x_train,y_train)\n",
    "    y_pred=gnb.predict(x_test)\n",
    "    print (metrics.accuracy_score(y_test, y_pred))\n",
    "    print (metrics.confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "#支持向量机\n",
    "def do_svm_wordbag(x_train, x_test, y_train, y_test):\n",
    "    #print (\"SVM and wordbag\")\n",
    "    clf = svm.SVC()\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print (metrics.accuracy_score(y_test, y_pred))\n",
    "    print (metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    \n",
    "#词袋模型-卷积神经网络\n",
    "def do_cnn_wordbag(trainX, testX, trainY, testY):\n",
    "    global max_document_length\n",
    "    #print (\"CNN and tf\")\n",
    "\n",
    "    trainX = pad_sequences(trainX, maxlen=max_document_length, value=0.) #对数据进行填充，不带最大长度的用0替换\n",
    "    testX = pad_sequences(testX, maxlen=max_document_length, value=0.)\n",
    "    # Converting labels to binary vectors\n",
    "    trainY = to_categorical(trainY, nb_classes=2)\n",
    "    testY = to_categorical(testY, nb_classes=2)\n",
    "\n",
    "    # Building convolutional network(change output_dim = 64 --> output_dim = 32,n_epoch= 5 --> n_epoch =1)\n",
    "    network = input_data(shape=[None,max_document_length], name='input')\n",
    "    network = tflearn.embedding(network, input_dim=10240000, output_dim=32)\n",
    "    branch1 = conv_1d(network, 32, 3, padding='valid', activation='relu', regularizer=\"L2\")\n",
    "    branch2 = conv_1d(network, 32, 4, padding='valid', activation='relu', regularizer=\"L2\")\n",
    "    branch3 = conv_1d(network, 32, 5, padding='valid', activation='relu', regularizer=\"L2\")\n",
    "    network = merge([branch1, branch2, branch3], mode='concat', axis=1)\n",
    "    network = tf.expand_dims(network, 2)\n",
    "    network = global_max_pool(network)\n",
    "    network = dropout(network, 0.8)\n",
    "    network = fully_connected(network, 2, activation='softmax')\n",
    "    network = regression(network, optimizer='adam', learning_rate=0.001,\n",
    "                         loss='categorical_crossentropy', name='target')\n",
    "    # Training\n",
    "    model = tflearn.DNN(network, tensorboard_verbose=0)\n",
    "    model.fit(trainX, trainY,\n",
    "              n_epoch=1, shuffle=True, validation_set=(testX, testY),\n",
    "              show_metric=True, batch_size=100,run_id=\"review\")\n",
    "\n",
    "    \n",
    "#词袋模型-rnn循环神经网络,output_dim =128 -->output_dim = 32,n_epoch=1\n",
    "def do_rnn_wordbag(trainX, testX, trainY, testY):\n",
    "    global max_document_length\n",
    "    #print (\"RNN and wordbag\")\n",
    "\n",
    "    trainX = pad_sequences(trainX, maxlen=max_document_length, value=0.)\n",
    "    testX = pad_sequences(testX, maxlen=max_document_length, value=0.)\n",
    "    # Converting labels to binary vectors\n",
    "    trainY = to_categorical(trainY, nb_classes=2)\n",
    "    testY = to_categorical(testY, nb_classes=2)\n",
    "\n",
    "    # Network building\n",
    "    net = tflearn.input_data([None, max_document_length])\n",
    "    net = tflearn.embedding(net, input_dim=10240000, output_dim=32)\n",
    "    net = tflearn.lstm(net, 32, dropout=0.8)\n",
    "    net = tflearn.fully_connected(net, 2, activation='softmax')\n",
    "    net = tflearn.regression(net, optimizer='adam', learning_rate=0.001,\n",
    "                             loss='categorical_crossentropy')\n",
    "\n",
    "    # Training\n",
    "    model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "    model.fit(trainX, trainY, validation_set=(testX, testY), show_metric=True,\n",
    "              batch_size=10,run_id=\"review\",n_epoch=1)\n",
    "\n",
    "#词袋模型-多层感知机模型   \n",
    "def do_dnn_wordbag(x_train, x_test, y_train, y_test):\n",
    "    #print (\"MLP and wordbag\")\n",
    "\n",
    "    # Building deep neural network\n",
    "    clf = MLPClassifier(solver='lbfgs',\n",
    "                        alpha=1e-5,\n",
    "                        hidden_layer_sizes = (5, 2),\n",
    "                        random_state = 1)\n",
    "    print  (clf)\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print (metrics.accuracy_score(y_test, y_pred))\n",
    "    print (metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('end_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "第三大部分：开始进行模型训练\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_features=1000\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/train/pos/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/train/neg/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/test/pos/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/test/neg/\n",
      "CountVectorizer(decode_error='ignore', max_features=1000, stop_words='english',\n",
      "                strip_accents='ascii')\n",
      "CountVectorizer(decode_error='ignore', stop_words='english',\n",
      "                strip_accents='ascii',\n",
      "                vocabulary={'10': 0, '100': 1, '15': 2, '20': 3, '30': 4,\n",
      "                            '50': 5, '80': 6, '90': 7, 'ability': 8, 'able': 9,\n",
      "                            'absolutely': 10, 'accent': 11, 'act': 12,\n",
      "                            'acted': 13, 'acting': 14, 'action': 15,\n",
      "                            'actor': 16, 'actors': 17, 'actress': 18,\n",
      "                            'actual': 19, 'actually': 20, 'add': 21,\n",
      "                            'admit': 22, 'adult': 23, 'adventure': 24,\n",
      "                            'age': 25, 'ago': 26, 'agree': 27, 'air': 28,\n",
      "                            'alive': 29, ...})\n",
      "max_features=3000\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/train/pos/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/train/neg/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/test/pos/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/test/neg/\n",
      "CountVectorizer(decode_error='ignore', max_features=3000, stop_words='english',\n",
      "                strip_accents='ascii')\n",
      "CountVectorizer(decode_error='ignore', stop_words='english',\n",
      "                strip_accents='ascii',\n",
      "                vocabulary={'000': 0, '10': 1, '100': 2, '11': 3, '12': 4,\n",
      "                            '13': 5, '14': 6, '15': 7, '16': 8, '17': 9,\n",
      "                            '18': 10, '1950': 11, '1950s': 12, '1970': 13,\n",
      "                            '1980': 14, '1st': 15, '20': 16, '2000': 17,\n",
      "                            '2001': 18, '2006': 19, '24': 20, '25': 21,\n",
      "                            '30': 22, '40': 23, '50': 24, '60': 25, '60s': 26,\n",
      "                            '70': 27, '70s': 28, '80': 29, ...})\n",
      "max_features=5000\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/train/pos/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/train/neg/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/test/pos/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/test/neg/\n",
      "CountVectorizer(decode_error='ignore', max_features=5000, stop_words='english',\n",
      "                strip_accents='ascii')\n",
      "CountVectorizer(decode_error='ignore', stop_words='english',\n",
      "                strip_accents='ascii',\n",
      "                vocabulary={'00': 0, '000': 1, '10': 2, '100': 3, '11': 4,\n",
      "                            '12': 5, '13': 6, '13th': 7, '14': 8, '15': 9,\n",
      "                            '16': 10, '17': 11, '18': 12, '1930': 13,\n",
      "                            '1930s': 14, '1933': 15, '1936': 16, '1940': 17,\n",
      "                            '1950': 18, '1950s': 19, '1960': 20, '1960s': 21,\n",
      "                            '1968': 22, '1970': 23, '1970s': 24, '1971': 25,\n",
      "                            '1972': 26, '1973': 27, '1978': 28, '1980': 29, ...})\n",
      "max_features=7000\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/train/pos/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/train/neg/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/test/pos/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/test/neg/\n",
      "CountVectorizer(decode_error='ignore', max_features=7000, stop_words='english',\n",
      "                strip_accents='ascii')\n",
      "CountVectorizer(decode_error='ignore', stop_words='english',\n",
      "                strip_accents='ascii',\n",
      "                vocabulary={'00': 0, '000': 1, '10': 2, '100': 3, '101': 4,\n",
      "                            '11': 5, '12': 6, '13': 7, '13th': 8, '14': 9,\n",
      "                            '15': 10, '16': 11, '17': 12, '18': 13, '19': 14,\n",
      "                            '1930': 15, '1930s': 16, '1933': 17, '1934': 18,\n",
      "                            '1936': 19, '1939': 20, '1940': 21, '1940s': 22,\n",
      "                            '1944': 23, '1945': 24, '1948': 25, '1950': 26,\n",
      "                            '1950s': 27, '1951': 28, '1953': 29, ...})\n",
      "max_features=9000\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/train/pos/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/train/neg/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/test/pos/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/test/neg/\n",
      "CountVectorizer(decode_error='ignore', max_features=9000, stop_words='english',\n",
      "                strip_accents='ascii')\n",
      "CountVectorizer(decode_error='ignore', stop_words='english',\n",
      "                strip_accents='ascii',\n",
      "                vocabulary={'00': 0, '000': 1, '10': 2, '100': 3, '1000': 4,\n",
      "                            '101': 5, '11': 6, '12': 7, '13': 8, '13th': 9,\n",
      "                            '14': 10, '15': 11, '16': 12, '17': 13, '18': 14,\n",
      "                            '19': 15, '1920': 16, '1920s': 17, '1930': 18,\n",
      "                            '1930s': 19, '1931': 20, '1932': 21, '1933': 22,\n",
      "                            '1934': 23, '1936': 24, '1938': 25, '1939': 26,\n",
      "                            '1940': 27, '1940s': 28, '1941': 29, ...})\n",
      "max_features=11000\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/train/pos/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/train/neg/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/test/pos/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/test/neg/\n",
      "CountVectorizer(decode_error='ignore', max_features=11000, stop_words='english',\n",
      "                strip_accents='ascii')\n",
      "CountVectorizer(decode_error='ignore', stop_words='english',\n",
      "                strip_accents='ascii',\n",
      "                vocabulary={'00': 0, '000': 1, '01': 2, '10': 3, '100': 4,\n",
      "                            '1000': 5, '101': 6, '11': 7, '11th': 8, '12': 9,\n",
      "                            '13': 10, '13th': 11, '14': 12, '15': 13, '150': 14,\n",
      "                            '16': 15, '17': 16, '18': 17, '180': 18, '18th': 19,\n",
      "                            '19': 20, '1920': 21, '1920s': 22, '1922': 23,\n",
      "                            '1928': 24, '1930': 25, '1930s': 26, '1931': 27,\n",
      "                            '1932': 28, '1933': 29, ...})\n",
      "max_features=13000\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/train/pos/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/train/neg/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/test/pos/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/test/neg/\n",
      "CountVectorizer(decode_error='ignore', max_features=13000, stop_words='english',\n",
      "                strip_accents='ascii')\n",
      "CountVectorizer(decode_error='ignore', stop_words='english',\n",
      "                strip_accents='ascii',\n",
      "                vocabulary={'00': 0, '000': 1, '01': 2, '10': 3, '100': 4,\n",
      "                            '1000': 5, '101': 6, '11': 7, '11th': 8, '12': 9,\n",
      "                            '13': 10, '13th': 11, '14': 12, '14th': 13,\n",
      "                            '15': 14, '150': 15, '16': 16, '17': 17, '18': 18,\n",
      "                            '180': 19, '18th': 20, '19': 21, '1920': 22,\n",
      "                            '1920s': 23, '1922': 24, '1928': 25, '1930': 26,\n",
      "                            '1930s': 27, '1931': 28, '1932': 29, ...})\n",
      "max_features=15000\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/train/pos/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/train/neg/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/test/pos/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/test/neg/\n",
      "CountVectorizer(decode_error='ignore', max_features=15000, stop_words='english',\n",
      "                strip_accents='ascii')\n",
      "CountVectorizer(decode_error='ignore', stop_words='english',\n",
      "                strip_accents='ascii',\n",
      "                vocabulary={'00': 0, '000': 1, '007': 2, '01': 3, '02': 4,\n",
      "                            '10': 5, '100': 6, '1000': 7, '101': 8, '102': 9,\n",
      "                            '10th': 10, '11': 11, '11th': 12, '12': 13,\n",
      "                            '120': 14, '13': 15, '13th': 16, '14': 17,\n",
      "                            '14th': 18, '15': 19, '150': 20, '16': 21,\n",
      "                            '16mm': 22, '17': 23, '17th': 24, '18': 25,\n",
      "                            '180': 26, '18th': 27, '19': 28, '1912': 29, ...})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_features=17000\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/train/pos/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/train/neg/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/test/pos/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/test/neg/\n",
      "CountVectorizer(decode_error='ignore', max_features=17000, stop_words='english',\n",
      "                strip_accents='ascii')\n",
      "CountVectorizer(decode_error='ignore', stop_words='english',\n",
      "                strip_accents='ascii',\n",
      "                vocabulary={'00': 0, '000': 1, '007': 2, '01': 3, '02': 4,\n",
      "                            '05': 5, '06': 6, '10': 7, '100': 8, '1000': 9,\n",
      "                            '101': 10, '102': 11, '10th': 12, '11': 13,\n",
      "                            '110': 14, '11th': 15, '12': 16, '120': 17,\n",
      "                            '12th': 18, '13': 19, '13th': 20, '14': 21,\n",
      "                            '140': 22, '14th': 23, '15': 24, '150': 25,\n",
      "                            '16': 26, '16mm': 27, '17': 28, '17th': 29, ...})\n",
      "max_features=19000\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/train/pos/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/train/neg/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/test/pos/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/test/neg/\n",
      "CountVectorizer(decode_error='ignore', max_features=19000, stop_words='english',\n",
      "                strip_accents='ascii')\n",
      "CountVectorizer(decode_error='ignore', stop_words='english',\n",
      "                strip_accents='ascii',\n",
      "                vocabulary={'00': 0, '000': 1, '007': 2, '01': 3, '02': 4,\n",
      "                            '05': 5, '06': 6, '07': 7, '08': 8, '10': 9,\n",
      "                            '100': 10, '1000': 11, '101': 12, '102': 13,\n",
      "                            '105': 14, '10th': 15, '11': 16, '110': 17,\n",
      "                            '11th': 18, '12': 19, '120': 20, '12th': 21,\n",
      "                            '13': 22, '13th': 23, '14': 24, '140': 25,\n",
      "                            '14th': 26, '15': 27, '150': 28, '16': 29, ...})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#通过贝叶斯模型的效果来对特征的个数做决定。\n",
    "\"\"\"\n",
    "选定超参数的个数,\n",
    "\"\"\"\n",
    "show_diffrent_max_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_time: 2021-11-07 11:12:16\n",
      "start get_features_by_wordbag --->\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/train/pos/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/train/neg/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/test/pos/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/test/neg/\n",
      "CountVectorizer(decode_error='ignore', max_features=200, stop_words='english',\n",
      "                strip_accents='ascii')\n",
      "CountVectorizer(decode_error='ignore', stop_words='english',\n",
      "                strip_accents='ascii',\n",
      "                vocabulary={'10': 0, 'acting': 1, 'action': 2, 'actor': 3,\n",
      "                            'actors': 4, 'actually': 5, 'american': 6,\n",
      "                            'audience': 7, 'away': 8, 'awful': 9, 'bad': 10,\n",
      "                            'beautiful': 11, 'believe': 12, 'best': 13,\n",
      "                            'better': 14, 'big': 15, 'bit': 16, 'black': 17,\n",
      "                            'book': 18, 'boring': 19, 'br': 20, 'budget': 21,\n",
      "                            'camera': 22, 'cast': 23, 'character': 24,\n",
      "                            'characters': 25, 'classic': 26, 'come': 27,\n",
      "                            'comedy': 28, 'comes': 29, ...})\n",
      "start NB\n",
      "0.74732\n",
      "[[10022  2478]\n",
      " [ 3839  8661]]\n",
      "start SVM\n",
      "0.7866\n",
      "[[10104  2396]\n",
      " [ 2939  9561]]\n",
      "start DNN\n",
      "MLPClassifier(alpha=1e-05, hidden_layer_sizes=(5, 2), random_state=1,\n",
      "              solver='lbfgs')\n",
      "0.78304\n",
      "[[9823 2677]\n",
      " [2747 9753]]\n",
      "end_time: 2021-11-07 11:17:41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3_5\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "三：模型训练\n",
    "   --词袋模型1\n",
    "   --max_feaure = 300\n",
    "   --max_document_length = 500\n",
    "   \n",
    "因为有将文档的最大长度减少，所以看到准确率是由稍微降低的\n",
    "\"\"\"\n",
    "max_features=200\n",
    "max_document_length=500  \n",
    "vocabulary=None\n",
    "import datetime\n",
    "print('start_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "\n",
    "print (\"start get_features_by_wordbag --->\")\n",
    "x_train, x_test, y_train, y_test=get_features_by_wordbag()\n",
    "\n",
    "#Nb\n",
    "print(\"start NB\")\n",
    "do_nb_wordbag(x_train, x_test, y_train, y_test)\n",
    "\n",
    "#SVM\n",
    "print(\"start SVM\")\n",
    "do_svm_wordbag(x_train, x_test, y_train, y_test)\n",
    "\n",
    "#多层感知机。\n",
    "print(\"start DNN\")\n",
    "do_dnn_wordbag(x_train, x_test, y_train, y_test)\n",
    "\n",
    "print('end_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 249  | total loss: \u001b[1m\u001b[32m0.69318\u001b[0m\u001b[0m | time: 6375.002s\n",
      "| Adam | epoch: 001 | loss: 0.69318 - acc: 0.5079 -- iter: 24900/25000\n",
      "Training Step: 250  | total loss: \u001b[1m\u001b[32m0.69319\u001b[0m\u001b[0m | time: 6416.163s\n",
      "| Adam | epoch: 001 | loss: 0.69319 - acc: 0.5062 | val_loss: 0.69314 - val_acc: 0.5000 -- iter: 25000/25000\n",
      "--\n",
      "end_time: 2021-11-07 13:07:33\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "三：模型训练\n",
    "   --词袋模型2\n",
    "   --max_feaure = 300\n",
    "   --max_document_length = 500\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "*从结果来看，CNN在词袋模型上效果表现不好\n",
    "*内存不够，不支持RNN模型训练，且词袋模型特征,不适合RNN,RNN适合处理有时序关系的序列。\n",
    "\"\"\"\n",
    "max_features=200\n",
    "max_document_length=500  \n",
    "vocabulary=None\n",
    "import datetime\n",
    "print('start_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "\n",
    "print (\"start get_features_by_wordbag --->\")\n",
    "x_train, x_test, y_train, y_test=get_features_by_wordbag()\n",
    "#卷积神经网络。\n",
    "print(\"start CNN\")\n",
    "do_cnn_wordbag(x_train, x_test, y_train, y_test)\n",
    "\n",
    "\n",
    "print('end_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_time: 2021-11-07 15:10:24\n",
      "start get_features_by_wordbag_tfidf --->\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/train/pos/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/train/neg/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/test/pos/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/test/neg/\n",
      "CountVectorizer(binary=True, decode_error='ignore', max_features=200,\n",
      "                stop_words='english', strip_accents='ascii')\n",
      "CountVectorizer(binary=True, decode_error='ignore', stop_words='english',\n",
      "                strip_accents='ascii',\n",
      "                vocabulary={'10': 0, 'acting': 1, 'action': 2, 'actor': 3,\n",
      "                            'actors': 4, 'actually': 5, 'american': 6,\n",
      "                            'audience': 7, 'away': 8, 'bad': 9, 'beautiful': 10,\n",
      "                            'believe': 11, 'best': 12, 'better': 13, 'big': 14,\n",
      "                            'bit': 15, 'black': 16, 'boring': 17, 'br': 18,\n",
      "                            'budget': 19, 'came': 20, 'camera': 21, 'cast': 22,\n",
      "                            'character': 23, 'characters': 24, 'classic': 25,\n",
      "                            'come': 26, 'comedy': 27, 'comes': 28,\n",
      "                            'completely': 29, ...})\n",
      "start NB\n",
      "0.75336\n",
      "[[9395 3105]\n",
      " [3061 9439]]\n",
      "start SVM\n",
      "0.77956\n",
      "[[9847 2653]\n",
      " [2858 9642]]\n",
      "start DNN\n",
      "MLPClassifier(alpha=1e-05, hidden_layer_sizes=(5, 2), random_state=1,\n",
      "              solver='lbfgs')\n",
      "0.5\n",
      "[[12500     0]\n",
      " [12500     0]]\n",
      "end_time: 2021-11-07 15:14:13\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "三：模型训练\n",
    "   --TF-IDF模型1\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "这里的DNN模型分类这么差\n",
    "max_feature = 200\n",
    "\"\"\"\n",
    "max_features=200\n",
    "max_document_length=500  \n",
    "vocabulary=None\n",
    "import datetime\n",
    "print('start_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "\n",
    "print (\"start get_features_by_wordbag_tfidf --->\")\n",
    "x_train, x_test, y_train, y_test=get_features_by_wordbag_tfidf()\n",
    "\n",
    "#Nb\n",
    "print(\"start NB\")\n",
    "do_nb_wordbag(x_train, x_test, y_train, y_test)\n",
    "\n",
    "#SVM\n",
    "print(\"start SVM\")\n",
    "do_svm_wordbag(x_train, x_test, y_train, y_test)\n",
    "\n",
    "#多层感知机。\n",
    "print(\"start DNN\")#这里的结果很奇怪\n",
    "do_dnn_wordbag(x_train, x_test, y_train, y_test)\n",
    "\n",
    "\n",
    "print('end_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 249  | total loss: \u001b[1m\u001b[32m0.69341\u001b[0m\u001b[0m | time: 1349.115s\n",
      "| Adam | epoch: 001 | loss: 0.69341 - acc: 0.4904 -- iter: 24900/25000\n",
      "Training Step: 250  | total loss: \u001b[1m\u001b[32m0.69344\u001b[0m\u001b[0m | time: 1358.802s\n",
      "| Adam | epoch: 001 | loss: 0.69344 - acc: 0.4883 | val_loss: 0.69318 - val_acc: 0.5000 -- iter: 25000/25000\n",
      "--\n",
      "end_time: 2021-11-07 15:38:02\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "三：模型训练\n",
    "   --TF-IDf模型2\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "有的时候会出现报错\n",
    "\"\"\"\n",
    "\n",
    "max_features=200\n",
    "max_document_length=500  \n",
    "vocabulary=None\n",
    "import datetime\n",
    "print('start_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "\n",
    "\n",
    "print (\"start get_features_by_wordbag_tfidf --->\")\n",
    "x_train, x_test, y_train, y_test=get_features_by_wordbag_tfidf()\n",
    "\n",
    "#卷积神经网络。\n",
    "print(\"start CNN\")\n",
    "do_cnn_wordbag(x_train, x_test, y_train, y_test)\n",
    "\n",
    "print('end_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_time: 2021-11-07 15:52:41\n",
      "start get_features_by_wordbag_tf --->\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/train/pos/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/train/neg/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/test/pos/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/test/neg/\n",
      "start NB\n",
      "0.5054\n",
      "[[9994 2506]\n",
      " [9859 2641]]\n",
      "start SVM\n",
      "0.53244\n",
      "[[8831 3669]\n",
      " [8020 4480]]\n",
      "start DNN\n",
      "MLPClassifier(alpha=1e-05, hidden_layer_sizes=(5, 2), random_state=1,\n",
      "              solver='lbfgs')\n",
      "0.49992\n",
      "[[    4 12496]\n",
      " [    6 12494]]\n",
      "end_time: 2021-11-07 15:59:24\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "三：模型训练\n",
    "   --TF(词汇表)模型1\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "max_feature = 200\n",
    "max_document_length = 500\n",
    "--可以试试RNN模型\n",
    "\"\"\"\n",
    "max_features=200\n",
    "max_document_length=500  \n",
    "vocabulary=None\n",
    "import datetime\n",
    "print('start_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "\n",
    "print (\"start get_features_by_wordbag_tf --->\")\n",
    "x_train, x_test, y_train, y_test=get_features_by_tf()\n",
    "\n",
    "#Nb\n",
    "print(\"start NB\")\n",
    "do_nb_wordbag(x_train, x_test, y_train, y_test)\n",
    "\n",
    "#SVM\n",
    "print(\"start SVM\")\n",
    "do_svm_wordbag(x_train, x_test, y_train, y_test)\n",
    "\n",
    "#多层感知机。\n",
    "print(\"start DNN\")#这里的结果很奇怪\n",
    "do_dnn_wordbag(x_train, x_test, y_train, y_test)\n",
    "\n",
    "\n",
    "print('end_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 249  | total loss: \u001b[1m\u001b[32m0.45247\u001b[0m\u001b[0m | time: 1075.724s\n",
      "| Adam | epoch: 001 | loss: 0.45247 - acc: 0.7988 -- iter: 24900/25000\n",
      "Training Step: 250  | total loss: \u001b[1m\u001b[32m0.45441\u001b[0m\u001b[0m | time: 1088.822s\n",
      "| Adam | epoch: 001 | loss: 0.45441 - acc: 0.7999 | val_loss: 0.45172 - val_acc: 0.7915 -- iter: 25000/25000\n",
      "--\n",
      "end_time: 2021-11-07 17:54:18\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "三：模型训练\n",
    "   --TF(词汇表)模型2\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "词汇表模型在CNN中准确率提高\n",
    "\"\"\"\n",
    "\n",
    "max_features=200\n",
    "max_document_length=500  \n",
    "vocabulary=None\n",
    "import datetime\n",
    "print('start_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "\n",
    "\n",
    "print (\"start get_features_by_wordbag_tf --->\")\n",
    "x_train, x_test, y_train, y_test=get_features_by_tf()\n",
    "\n",
    "#卷积神经网络。\n",
    "print(\"start CNN\")\n",
    "do_cnn_wordbag(x_train, x_test, y_train, y_test)\n",
    "\n",
    "print('end_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2499  | total loss: \u001b[1m\u001b[32m0.44266\u001b[0m\u001b[0m | time: 9313.435s\n",
      "| Adam | epoch: 001 | loss: 0.44266 - acc: 0.7940 -- iter: 24990/25000\n",
      "Training Step: 2500  | total loss: \u001b[1m\u001b[32m0.41783\u001b[0m\u001b[0m | time: 9321.477s\n",
      "| Adam | epoch: 001 | loss: 0.41783 - acc: 0.8146 | val_loss: 0.58620 - val_acc: 0.6940 -- iter: 25000/25000\n",
      "--\n",
      "end_time: 2021-11-08 00:53:29\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "三：模型训练\n",
    "   --TF(词汇表)模型2\n",
    "\"\"\"RNN\n",
    "\"\"\"\n",
    "词汇表模型在RNN中准确率提高\n",
    "\"\"\"\n",
    "\n",
    "max_features=200\n",
    "max_document_length=20 \n",
    "vocabulary=None\n",
    "import datetime\n",
    "print('start_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "\n",
    "\n",
    "print (\"start get_features_by_wordbag_tf --->\")\n",
    "x_train, x_test, y_train, y_test=get_features_by_tf()\n",
    "\n",
    "#卷积神经网络。\n",
    "print(\"start RNN\")\n",
    "do_rnn_wordbag(x_train, x_test, y_train, y_test)\n",
    "\n",
    "print('end_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2499  | total loss: \u001b[1m\u001b[32m0.29760\u001b[0m\u001b[0m | time: 9464.104s\n",
      "| Adam | epoch: 001 | loss: 0.29760 - acc: 0.9001 -- iter: 24990/25000\n",
      "Training Step: 2500  | total loss: \u001b[1m\u001b[32m0.29589\u001b[0m\u001b[0m | time: 9477.273s\n",
      "| Adam | epoch: 001 | loss: 0.29589 - acc: 0.9001 | val_loss: 0.51490 - val_acc: 0.7622 -- iter: 25000/25000\n",
      "--\n",
      "end_time: 2021-11-08 12:10:00\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "三：模型训练\n",
    "   --TF(词汇表)模型2\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "词汇表模型在RNN中准确率提高\n",
    "\"\"\"\n",
    "\n",
    "max_features=200\n",
    "max_document_length=50\n",
    "vocabulary=None\n",
    "import datetime\n",
    "print('start_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "\n",
    "\n",
    "print (\"start get_features_by_wordbag_tf --->\")\n",
    "x_train, x_test, y_train, y_test=get_features_by_tf()\n",
    "\n",
    "#卷积神经网络。\n",
    "print(\"start RNN\")\n",
    "do_rnn_wordbag(x_train, x_test, y_train, y_test)\n",
    "\n",
    "print('end_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2499  | total loss: \u001b[1m\u001b[32m0.32142\u001b[0m\u001b[0m | time: 10755.006s\n",
      "| Adam | epoch: 001 | loss: 0.32142 - acc: 0.8877 -- iter: 24990/25000\n",
      "Training Step: 2500  | total loss: \u001b[1m\u001b[32m0.33232\u001b[0m\u001b[0m | time: 10772.556s\n",
      "| Adam | epoch: 001 | loss: 0.33232 - acc: 0.8789 | val_loss: 0.45650 - val_acc: 0.7992 -- iter: 25000/25000\n",
      "--\n",
      "end_time: 2021-11-08 15:28:53\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "三：模型训练\n",
    "   --TF(词汇表)模型2\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "词汇表模型在RNN中准确率提高\n",
    "\"\"\"\n",
    "\n",
    "max_features=200\n",
    "max_document_length=100\n",
    "vocabulary=None\n",
    "import datetime\n",
    "print('start_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "\n",
    "\n",
    "print (\"start get_features_by_wordbag_tf --->\")\n",
    "x_train, x_test, y_train, y_test=get_features_by_tf()\n",
    "\n",
    "#卷积神经网络。\n",
    "print(\"start RNN\")\n",
    "do_rnn_wordbag(x_train, x_test, y_train, y_test)\n",
    "\n",
    "print('end_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2499  | total loss: \u001b[1m\u001b[32m0.69299\u001b[0m\u001b[0m | time: 9632.946s\n",
      "| Adam | epoch: 001 | loss: 0.69299 - acc: 0.5219 -- iter: 24990/25000\n",
      "Training Step: 2500  | total loss: \u001b[1m\u001b[32m0.69220\u001b[0m\u001b[0m | time: 9697.120s\n",
      "| Adam | epoch: 001 | loss: 0.69220 - acc: 0.5597 | val_loss: 0.69294 - val_acc: 0.5058 -- iter: 25000/25000\n",
      "--\n",
      "end_time: 2021-11-09 02:54:58\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "三：模型训练\n",
    "   --TF(词汇表)模型2\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "词汇表模型在RNN中准确率提高\n",
    "\"\"\"\n",
    "\n",
    "max_features=200\n",
    "max_document_length=500\n",
    "vocabulary=None\n",
    "import datetime\n",
    "print('start_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "\n",
    "\n",
    "print (\"start get_features_by_wordbag_tf --->\")\n",
    "x_train, x_test, y_train, y_test=get_features_by_tf()\n",
    "\n",
    "#卷积神经网络。\n",
    "print(\"start RNN\")\n",
    "do_rnn_wordbag(x_train, x_test, y_train, y_test)\n",
    "\n",
    "print('end_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_time: 2021-11-09 18:43:33\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/train/pos/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/train/neg/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/test/pos/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/test/neg/\n",
      "end_time: 2021-11-09 18:51:52\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.20512178,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.11070832,\n",
       "        0.1216088 ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "补充：\n",
    "通过TF-IDF提取文本特征的第二种方法(新)\n",
    "直接通过TF-IDF模型构建文本特征\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "print('start_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def get_features_by_tfidf():\n",
    "    x_train, x_test, y_train, y_test=load_all_files()\n",
    "    #all_text = pd.concat([pd.DataFrame(x_train), pd.DataFrame(x_test)])\n",
    "    all_text = x_train + x_test\n",
    "    \n",
    "    word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'-?\\w{1,}', #通过-?来考虑正负号的顺序\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=200)\n",
    "    \n",
    "    word_vectorizer.fit(all_text)\n",
    "    x_train = word_vectorizer.transform(x_train).toarray()\n",
    "    x_test = word_vectorizer.transform(x_test).toarray() \n",
    "    \n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "x_train, x_test, y_train, y_test = get_features_by_tfidf()\n",
    "print('end_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))\n",
    "x_train\n",
    "#shape(2500,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_time: 2021-11-09 16:02:00\n",
      "start get_features_by_tfidf(new kind of tf-idf)--->\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/train/pos/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/train/neg/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/test/pos/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/test/neg/\n",
      "start NB\n",
      "0.76276\n",
      "[[9641 2859]\n",
      " [3072 9428]]\n",
      "start SVM\n",
      "0.79096\n",
      "[[10024  2476]\n",
      " [ 2750  9750]]\n",
      "start DNN\n",
      "MLPClassifier(alpha=1e-05, hidden_layer_sizes=(5, 2), random_state=1,\n",
      "              solver='lbfgs')\n",
      "0.5\n",
      "[[12500     0]\n",
      " [12500     0]]\n",
      "end_time: 2021-11-09 16:05:50\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "三：模型训练\n",
    "   --TF-IDF(新)模型2\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "max_feature = 200\n",
    "max_document_length = 500\n",
    "\"\"\"\n",
    "max_features=200\n",
    "max_document_length=500  \n",
    "vocabulary=None\n",
    "import datetime\n",
    "print('start_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "print (\"start get_features_by_tfidf(new kind of tf-idf)--->\")\n",
    "x_train, x_test, y_train, y_test=get_features_by_tfidf()\n",
    "\n",
    "#Nb\n",
    "print(\"start NB\")\n",
    "do_nb_wordbag(x_train, x_test, y_train, y_test)\n",
    "\n",
    "#SVM\n",
    "print(\"start SVM\")\n",
    "do_svm_wordbag(x_train, x_test, y_train, y_test)\n",
    "\n",
    "#多层感知机。\n",
    "print(\"start DNN\")#这里的结果很奇怪\n",
    "do_dnn_wordbag(x_train, x_test, y_train, y_test)\n",
    "\n",
    "\n",
    "print('end_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 249  | total loss: \u001b[1m\u001b[32m0.69309\u001b[0m\u001b[0m | time: 1307.355s\n",
      "| Adam | epoch: 001 | loss: 0.69309 - acc: 0.5108 -- iter: 24900/25000\n",
      "Training Step: 250  | total loss: \u001b[1m\u001b[32m0.69310\u001b[0m\u001b[0m | time: 1321.471s\n",
      "| Adam | epoch: 001 | loss: 0.69310 - acc: 0.5097 | val_loss: 0.69316 - val_acc: 0.5000 -- iter: 25000/25000\n",
      "--\n",
      "end_time: 2021-11-09 16:53:40\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "三：模型训练\n",
    "   --TF-IDF(新)模型2\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "CNN模型训练\n",
    "\"\"\"\n",
    "\n",
    "max_features=200\n",
    "max_document_length=500  \n",
    "vocabulary=None\n",
    "import datetime\n",
    "print('start_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "print (\"start get_features_by_tfidf(new kind of tf-idf)--->\")\n",
    "x_train, x_test, y_train, y_test=get_features_by_tfidf()\n",
    "\n",
    "#卷积神经网络。\n",
    "print(\"start CNN\")\n",
    "do_cnn_wordbag(x_train, x_test, y_train, y_test)\n",
    "\n",
    "print('end_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2499  | total loss: \u001b[1m\u001b[32m0.69504\u001b[0m\u001b[0m | time: 15527.173s\n",
      "| Adam | epoch: 001 | loss: 0.69504 - acc: 0.4795 -- iter: 24990/25000\n",
      "Training Step: 2500  | total loss: \u001b[1m\u001b[32m0.69472\u001b[0m\u001b[0m | time: 15540.990s\n",
      "| Adam | epoch: 001 | loss: 0.69472 - acc: 0.4916 | val_loss: 0.69341 - val_acc: 0.5000 -- iter: 25000/25000\n",
      "--\n",
      "end_time: 2021-11-09 23:19:52\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "三：模型训练\n",
    "   --TF-IDF(新) 模型2\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "RNN模型训练\n",
    "\"\"\"\n",
    "\n",
    "max_features=200\n",
    "max_document_length=50  \n",
    "vocabulary=None\n",
    "import datetime\n",
    "print('start_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "print (\"start get_features_by_tfidf(new kind of tf-idf)--->\")\n",
    "x_train, x_test, y_train, y_test=get_features_by_tfidf()\n",
    "\n",
    "#卷积神经网络。\n",
    "print(\"start RNN\")\n",
    "do_rnn_wordbag(x_train, x_test, y_train, y_test)\n",
    "\n",
    "print('end_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
