{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***负面评论识别：用word2vec，及doc2vec提取特征\n",
    "   通过word2vec/doc2vec,我们既可以获取一维特征向量，也可以获取二维的特征向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_time: 2021-11-17 18:39:58\n",
      "end_time: 2021-11-17 18:39:58\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "第一步：导入相关的包\n",
    "\n",
    "这里我们换安装了一个较低版本的gensim，就没有报错了\n",
    "\"\"\"\n",
    "import datetime\n",
    "print('start_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import os\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_1d, global_max_pool\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.merge_ops import merge\n",
    "from tflearn.layers.estimator import regression\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from tflearn.layers.normalization import local_response_normalization\n",
    "from tensorflow.contrib import learn\n",
    "import gensim\n",
    "import re\n",
    "from collections import namedtuple\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec,LabeledSentence\n",
    "from random import shuffle\n",
    "import multiprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.metrics import classification_report\n",
    "import xgboost as xgb\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "max_features=200\n",
    "max_document_length=500\n",
    "vocabulary=None\n",
    "doc2ver_bin=\"doc23ver.bin\"\n",
    "word2ver_bin=\"word23ver.bin\"\n",
    "#LabeledSentence = gensim.models.doc2vec.LabeledSentence\n",
    "SentimentDocument = namedtuple('SentimentDocument', 'words tags')\n",
    "\n",
    "print('end_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_time: 2021-11-17 18:39:59\n",
      "end_time: 2021-11-17 18:39:59\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "第二步：读取文件数据,\n",
    "\n",
    "看起来像是将一个文档(文件)加载到一个字符串,然后再保存到列表中。\n",
    "\"\"\"\n",
    "print('start_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "def load_one_file(filename):\n",
    "    x=\"\"\n",
    "    with open(filename,encoding='gb18030', errors='ignore') as f:\n",
    "        for line in f:\n",
    "            line=line.strip('\\n')\n",
    "            line = line.strip('\\r')\n",
    "            x+=line\n",
    "    f.close()\n",
    "    return x\n",
    "\n",
    "def load_files_from_dir(rootdir):\n",
    "    x=[]\n",
    "    list = os.listdir(rootdir)\n",
    "    for i in range(0, len(list)):\n",
    "        path = os.path.join(rootdir, list[i])\n",
    "        if os.path.isfile(path):\n",
    "            v=load_one_file(path)\n",
    "            x.append(v)\n",
    "    return x\n",
    "\n",
    "def load_all_files():\n",
    "    x_train=[]\n",
    "    y_train=[]\n",
    "    x_test=[]\n",
    "    y_test=[]\n",
    "    path=\"E:/pycharm_project/deep_learning_web_safe/review/data/train/pos/\"\n",
    "    print (\"Load %s\" % path)\n",
    "    x_train=load_files_from_dir(path)\n",
    "    y_train=[0]*len(x_train)\n",
    "    path=\"E:/pycharm_project/deep_learning_web_safe/review/data/train/neg/\"\n",
    "    print (\"Load %s\" % path)\n",
    "    tmp=load_files_from_dir(path)\n",
    "    y_train+=[1]*len(tmp)\n",
    "    x_train+=tmp\n",
    "\n",
    "    path=\"E:/pycharm_project/deep_learning_web_safe/review/data/test/pos/\"\n",
    "    print (\"Load %s\" % path)\n",
    "    x_test=load_files_from_dir(path)\n",
    "    y_test=[0]*len(x_test)\n",
    "    path=\"E:/pycharm_project/deep_learning_web_safe/review/data/test/neg/\"\n",
    "    print (\"Load %s\" % path)\n",
    "    tmp=load_files_from_dir(path)\n",
    "    y_test+=[1]*len(tmp)\n",
    "    x_test+=tmp\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "print('end_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_time: 2021-11-17 18:41:21\n",
      "end_time: 2021-11-17 18:41:21\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "第三步：定义训练模型\n",
    "分为机器学习与深度学习模型两部分\n",
    "\"\"\"\n",
    "print('start_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "#下面的是机器学习模型\n",
    "def do_svm_doc2vec(x_train, x_test, y_train, y_test):\n",
    "    #print (\"SVM and doc2vec\")\n",
    "    clf = svm.SVC()\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print (metrics.accuracy_score(y_test, y_pred))\n",
    "    print (metrics.confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "def do_nb_doc2vec(x_train, x_test, y_train, y_test):\n",
    "    #print (\"NB and doc2vec\")\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(x_train,y_train)\n",
    "    y_pred=gnb.predict(x_test)\n",
    "    print (metrics.accuracy_score(y_test, y_pred))\n",
    "    print (metrics.confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "def do_rf_doc2vec(x_train, x_test, y_train, y_test):\n",
    "    #print (\"rf and doc2vec\")\n",
    "    clf = RandomForestClassifier(n_estimators=10)\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print (metrics.accuracy_score(y_test, y_pred))\n",
    "    print (metrics.confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "   \n",
    "#下面的是深度学习模型  \n",
    "#卷积神经网络二元\n",
    "def do_cnn_doc2vec_2d(trainX, testX, trainY, testY):\n",
    "    #print (\"CNN and doc2vec 2d\")\n",
    "\n",
    "    trainX = trainX.reshape([-1, max_features, max_document_length, 1])\n",
    "    testX = testX.reshape([-1, max_features, max_document_length, 1])\n",
    "    #对y处理得这段代码自己添加的。\n",
    "    y_train_one_hot = []\n",
    "    y_test_one_hot = []\n",
    "    for i in range(len(trainY)):\n",
    "        if ((i+1)%500==0 and trainY[i]== 0):\n",
    "            y_train_one_hot.append([float(1),float(0)])\n",
    "        elif ((i+1)%500==0 and trainY[i] == 1):\n",
    "            y_train_one_hot.append([float(0),float(1)])  \n",
    "        \n",
    "    for i in range(len(testY)):\n",
    "        if ((i+1)%500==0 and testY[i]== 0):\n",
    "            y_test_one_hot.append([float(1),float(0)])\n",
    "        elif ((i+1)%500==0 and testY[i] == 1):\n",
    "            y_test_one_hot.append([float(0),float(1)])  \n",
    "        \n",
    "    trainY = np.array(y_train_one_hot)#\n",
    "    testY = np.array(y_test_one_hot)\n",
    "\n",
    "\n",
    "    # Building convolutional network\n",
    "    network = input_data(shape=[None, max_features, max_document_length, 1], name='input')\n",
    "    network = conv_2d(network, 16, 3, activation='relu', regularizer=\"L2\")\n",
    "    network = max_pool_2d(network, 2)\n",
    "    network = local_response_normalization(network)\n",
    "    network = conv_2d(network, 32, 3, activation='relu', regularizer=\"L2\")\n",
    "    network = max_pool_2d(network, 2)\n",
    "    network = local_response_normalization(network)\n",
    "    network = fully_connected(network, 128, activation='tanh')\n",
    "    network = dropout(network, 0.8)\n",
    "    network = fully_connected(network, 256, activation='tanh')\n",
    "    network = dropout(network, 0.8)\n",
    "    #network = fully_connected(network, 10, activation='softmax')\n",
    "    network = fully_connected(network, 2, activation='softmax')\n",
    "    network = regression(network, optimizer='adam', learning_rate=0.01,\n",
    "                         loss='categorical_crossentropy', name='target')\n",
    "\n",
    "    # Training\n",
    "    model = tflearn.DNN(network, tensorboard_verbose=0)\n",
    "    model.fit({'input': trainX}, {'target': trainY}, n_epoch=20,\n",
    "               validation_set=({'input': testX}, {'target': testY}),\n",
    "               snapshot_step=100, show_metric=True, run_id='review')\n",
    "\n",
    "    #卷积神经网络：一元\n",
    "def do_cnn_doc2vec(trainX, testX, trainY, testY):\n",
    "    global max_features\n",
    "    #print (\"CNN and doc2vec\")\n",
    "\n",
    "    #trainX = pad_sequences(trainX, maxlen=max_features, value=0.)\n",
    "    #testX = pad_sequences(testX, maxlen=max_features, value=0.)\n",
    "    # Converting labels to binary vectors\n",
    "    trainY = to_categorical(trainY, nb_classes=2)\n",
    "    testY = to_categorical(testY, nb_classes=2)\n",
    "\n",
    "    # Building convolutional network\n",
    "    network = input_data(shape=[None,max_features], name='input')  #注意这里的维度是一维向量\n",
    "    network = tflearn.embedding(network, input_dim=1000000, output_dim=128,validate_indices=False)\n",
    "    branch1 = conv_1d(network, 128, 3, padding='valid', activation='relu', regularizer=\"L2\")\n",
    "    branch2 = conv_1d(network, 128, 4, padding='valid', activation='relu', regularizer=\"L2\")\n",
    "    branch3 = conv_1d(network, 128, 5, padding='valid', activation='relu', regularizer=\"L2\")\n",
    "    network = merge([branch1, branch2, branch3], mode='concat', axis=1)\n",
    "    network = tf.expand_dims(network, 2)\n",
    "    network = global_max_pool(network)\n",
    "    network = dropout(network, 0.8)\n",
    "    network = fully_connected(network, 2, activation='softmax')\n",
    "    network = regression(network, optimizer='adam', learning_rate=0.001,\n",
    "                         loss='categorical_crossentropy', name='target')\n",
    "    # Training\n",
    "    model = tflearn.DNN(network, tensorboard_verbose=0)\n",
    "    model.fit(trainX, trainY,\n",
    "              n_epoch=5, shuffle=True, validation_set=(testX, testY),\n",
    "              show_metric=True, batch_size=100,run_id=\"review\")\n",
    "\n",
    "\n",
    "#多层感知机模型(最简单的全连接神经网络)\n",
    "def do_dnn_doc2vec(x_train, x_test, y_train, y_test):\n",
    "    #print (\"MLP and doc2vec\")\n",
    "    global max_features\n",
    "    # Building deep neural network\n",
    "    clf = MLPClassifier(solver='lbfgs',\n",
    "                        alpha=1e-5,\n",
    "                        hidden_layer_sizes = (5, 2),\n",
    "                        random_state = 1)\n",
    "    print  (clf)\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print (metrics.accuracy_score(y_test, y_pred))\n",
    "    print (metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('end_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_time: 2021-11-17 18:41:24\n",
      "end_time: 2021-11-17 18:41:24\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "第四步：获取word2vec/doc2vec文本特征的预处理工作\n",
    "\"\"\"\n",
    "print('start_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "#对特殊符号与标点符号做处理\n",
    "def cleanText(corpus):\n",
    "    punctuation = \"\"\".,?!:;(){}[]\"\"\"\n",
    "    corpus = [z.lower().replace('\\n', '') for z in corpus]\n",
    "    corpus = [z.replace('<br />', ' ') for z in corpus]\n",
    "\n",
    "    # treat punctuation as individual words\n",
    "    for c in punctuation:\n",
    "        corpus = [z.replace(c, ' %s ' % c) for z in corpus]\n",
    "    corpus = [z.split() for z in corpus]\n",
    "    return corpus\n",
    "\n",
    "# Convert text to lower-case and strip punctuation/symbols from words\n",
    "def normalize_text(text):\n",
    "    norm_text = text.lower()\n",
    "\n",
    "    # Replace breaks with spaces\n",
    "    norm_text = norm_text.replace('<br />', ' ')\n",
    "\n",
    "    # Pad punctuation with spaces on both sides 在标点符号的两侧加空格\n",
    "    for char in ['.', '\"', ',', '(', ')', '!', '?', ';', ':']:\n",
    "        norm_text = norm_text.replace(char, ' ' + char + ' ')\n",
    "\n",
    "    return norm_text\n",
    "\n",
    "#doc2vec需要给每个reviewoc2Vec处理的每个英文段落，需要使用一个唯一的标识来标记\n",
    "def labelizeReviews(reviews, label_type):\n",
    "    labelized = []\n",
    "    for i, v in enumerate(reviews):\n",
    "        label = '%s_%s' % (label_type, i)\n",
    "        #labelized.append(LabeledSentence(v, [label]))\n",
    "        #labelized.append(LabeledSentence(words=v,tags=label))\n",
    "        labelized.append(SentimentDocument(v, [label]))\n",
    "    return labelized\n",
    "\n",
    "def getVecs(model, corpus, size):\n",
    "    vecs = [np.array(model.docvecs[z.tags[0]]).reshape((1, size)) for z in corpus]\n",
    "    return np.array(np.concatenate(vecs),dtype='float')\n",
    "\n",
    "\n",
    "def buildWordVector(imdb_w2v,text, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in text:\n",
    "        try:\n",
    "            vec += imdb_w2v[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    "\n",
    "print('end_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_time: 2021-11-12 15:51:16\n",
      "get_features_by_word2vec\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/train/pos/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/train/neg/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/test/pos/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/test/neg/\n",
      "Find cache file review_word2vec.bin\n",
      "end_time: 2021-11-12 15:55:00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-3.93052335e-01, -6.18693679e-01, -3.66564747e-01,  1.94379200e+00,\n",
       "        5.17187478e-02,  1.77266765e+00, -1.13460904e+00, -3.13377797e-01,\n",
       "       -2.10513102e-03, -6.58536136e-01, -2.33050575e+00, -1.01432710e+00,\n",
       "        8.28339694e-01,  2.11758228e+00,  1.28951850e+00, -6.75243553e-01,\n",
       "       -1.50628626e+00,  1.99503177e+00,  1.65627989e-01,  6.16613498e-01,\n",
       "        4.73955577e-01, -1.25254573e-01, -1.11096287e+00, -5.07561825e-01,\n",
       "        2.00181772e-01, -1.72721201e-01, -5.33310333e-01, -1.74073466e+00,\n",
       "       -1.83294122e-02, -1.83414718e+00, -2.01485962e+00,  1.43616146e+00,\n",
       "       -5.00532050e-01, -1.03555173e+00, -2.40348199e-01,  1.39355964e-01,\n",
       "       -3.89994850e-01,  6.15300557e-01,  1.37428103e+00, -1.28621961e+00,\n",
       "        5.47313351e-02,  9.20719203e-01, -6.83533426e-02,  1.74424609e+00,\n",
       "       -1.22915304e-01,  8.18166474e-01, -6.12105775e-01,  1.64017432e+00,\n",
       "       -4.40652735e-01, -4.29670367e-01, -4.86410967e-01, -4.90008247e-01,\n",
       "        6.27448884e-01, -1.47204810e+00, -1.82726443e+00,  9.27729255e-02,\n",
       "       -9.12050315e-04,  1.04294786e+00,  2.11050550e+00,  1.76903348e+00,\n",
       "       -2.56599626e-01, -8.02185618e-02,  1.06850744e+00, -9.54912216e-01,\n",
       "        5.06409146e-01,  1.02876753e+00,  2.07293961e-02, -3.39960266e-01,\n",
       "       -8.54409943e-02, -7.05812016e-02, -3.39286219e-01, -1.42709738e-01,\n",
       "       -1.54660500e+00, -4.78170213e-01,  2.22506070e-01, -3.06994610e-01,\n",
       "       -8.30096304e-01, -7.01073295e-01, -2.33717206e+00, -3.08830878e-01,\n",
       "       -1.22665418e+00, -1.26660269e+00, -6.47156951e-01,  5.35749188e-01,\n",
       "        4.02727469e-01, -1.94719148e-01, -3.99909114e-01, -1.07934201e+00,\n",
       "        1.02544600e+00,  1.89554394e+00, -8.34312300e-01, -7.81194073e-01,\n",
       "       -1.46392789e-01, -5.55334816e-01, -1.54044969e-01, -7.21865508e-01,\n",
       "       -1.57544299e+00,  2.79814741e-01,  6.32111787e-01,  2.32200749e-01,\n",
       "       -7.56775860e-01,  5.63007825e-01, -9.85628881e-01, -2.10966271e-01,\n",
       "        3.34983862e-01,  6.76967306e-01, -1.08750231e+00,  2.14255754e-01,\n",
       "        6.04348837e-01,  1.96216244e+00, -2.26778587e+00,  7.05537391e-01,\n",
       "       -6.12050474e-01,  6.22906860e-01,  1.12290437e+00,  1.70747838e+00,\n",
       "       -1.10953656e+00, -8.26933981e-01,  7.77888846e-01, -1.41673347e+00,\n",
       "       -2.85460675e-02, -4.50215603e-01,  5.36550854e-01,  4.83721024e-01,\n",
       "       -3.11492703e-01,  7.85986842e-01,  6.30728300e-01,  6.98593723e-01,\n",
       "        4.80017860e-01, -1.05849512e+00,  7.80244001e-01, -1.70348604e+00,\n",
       "       -1.38022073e-01,  1.34851030e+00, -7.45092732e-01, -1.82374438e-01,\n",
       "        1.54290260e+00, -1.76532613e+00,  9.93260485e-01, -4.98957189e-01,\n",
       "       -1.54944505e+00, -1.06937842e+00,  1.31553208e+00,  4.01298842e-01,\n",
       "       -4.40413151e-01, -3.84324650e-01,  5.96182806e-01,  3.02903715e-01,\n",
       "        9.46177377e-01,  2.07809450e+00, -7.95213703e-01,  1.41578700e+00,\n",
       "        1.50318376e+00,  7.86272119e-01,  1.58561154e-01, -1.08833510e+00,\n",
       "        1.33328995e+00, -3.38423494e-01,  4.46673518e-01,  1.28678808e+00,\n",
       "       -8.36543827e-02, -1.85334179e-01,  7.09791650e-01,  2.72772032e-01,\n",
       "        8.60950219e-01, -4.94131985e-01,  3.46566803e-01, -1.18626174e+00,\n",
       "       -4.28216873e-01, -1.08565281e+00, -1.20697886e+00,  2.67353589e-01,\n",
       "        2.97432955e-01, -9.82070803e-01, -1.13833222e+00, -6.08149116e-01,\n",
       "        9.31584295e-02, -5.97245764e-01, -7.80327736e-01,  1.17993759e+00,\n",
       "        9.50024169e-01, -9.34588160e-01,  7.12626528e-01, -4.03176224e-01,\n",
       "       -4.45759622e-01,  1.69699036e+00,  3.11235628e-01,  6.68780427e-01,\n",
       "       -3.09164243e-01, -7.58588744e-01,  1.27719535e+00, -6.97192936e-01,\n",
       "        1.10059001e-01,  5.47628820e-01,  2.25466161e+00, -1.99197379e-02,\n",
       "        6.20801235e-01,  9.60564542e-01, -5.62469427e-01,  1.09995910e+00])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "第五步：获取文本特征\n",
    "       开始训练word2vec模型\n",
    "       取值存在负数\n",
    "\"\"\"\n",
    "word2ver_bin = \"review_word2vec.bin\"\n",
    "\n",
    "print('start_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))\n",
    "#word2vec文本特征模型会将每一个词语转化为n维向量，所以内存会不够\n",
    "def  get_features_by_word2vec():\n",
    "    global  max_features\n",
    "    global word2ver_bin\n",
    "    x_train, x_test, y_train, y_test=load_all_files()\n",
    "\n",
    "    x_train=cleanText(x_train)\n",
    "    x_test=cleanText(x_test)\n",
    "\n",
    "    x=x_train+x_test\n",
    "    cores=multiprocessing.cpu_count() #获取当前计算机的cpu个数\n",
    "\n",
    "    if os.path.exists(word2ver_bin):\n",
    "        print (\"Find cache file %s\" % word2ver_bin)\n",
    "        model=gensim.models.Word2Vec.load(word2ver_bin)\n",
    "    else:\n",
    "        model=gensim.models.Word2Vec(size=max_features, window=10, min_count=1, iter=60, workers=cores)\n",
    "\n",
    "        model.build_vocab(x)\n",
    "\n",
    "        model.train(x, total_examples=model.corpus_count, epochs=model.iter)\n",
    "        model.save(word2ver_bin)\n",
    "\n",
    "\n",
    "    x_train= np.concatenate([buildWordVector(model,z, max_features) for z in x_train])\n",
    "    x_train = scale(x_train)\n",
    "    x_test= np.concatenate([buildWordVector(model,z, max_features) for z in x_test])\n",
    "    x_test = scale(x_test)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "print (\"get_features_by_word2vec\")\n",
    "x_train, x_test, y_train, y_test=get_features_by_word2vec()\n",
    "print('end_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))\n",
    "x_train[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_time: 2021-11-12 15:55:00\n",
      "get_features_by_word2vec_cnn_1d\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/train/pos/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/train/neg/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/test/pos/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/test/neg/\n",
      "Find cache file review_word2vec_cnn1d.bin\n",
      "end_time: 2021-11-12 15:55:47\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.39258288, 0.38558334, 0.35173874, 0.76502889, 0.28214877,\n",
       "       0.55568782, 0.63172517, 0.1241633 , 0.30440861, 0.31939899,\n",
       "       0.31816324, 0.49371318, 0.5394323 , 0.7197188 , 0.15799385,\n",
       "       0.31364137, 0.30582918, 0.43460088, 0.40009477, 0.75454824,\n",
       "       0.55829589, 0.30142037, 0.40112753, 0.30778506, 0.80458636,\n",
       "       0.4512612 , 0.50565783, 0.21746247, 0.7071669 , 0.2708229 ,\n",
       "       0.4270079 , 0.63145101, 0.27639013, 0.39352361, 0.77392144,\n",
       "       0.60174232, 0.44531788, 0.35229482, 0.47669598, 0.70511187,\n",
       "       0.71569787, 0.65529092, 0.73051261, 0.6434533 , 0.72184362,\n",
       "       0.42177348, 0.6172958 , 0.41333111, 0.37764418, 0.34879563,\n",
       "       0.21575099, 0.24806032, 0.35082055, 0.71397961, 0.26428397,\n",
       "       0.66022163, 0.37270537, 0.4946965 , 0.72343404, 0.57885381,\n",
       "       0.47149924, 0.61474078, 0.61860526, 0.36730958, 0.47656038,\n",
       "       0.45561113, 0.56377503, 0.71573091, 0.30952602, 0.55439629,\n",
       "       0.55267237, 0.42531454, 0.30238702, 0.40555068, 0.44835667,\n",
       "       0.54318113, 0.49287654, 0.18480698, 0.25169841, 0.33963222,\n",
       "       0.37199667, 0.31363738, 0.53133959, 0.40108949, 0.54537411,\n",
       "       0.31735149, 0.46992333, 0.45802568, 0.24406172, 0.65979189,\n",
       "       0.46747656, 0.28298343, 0.45847964, 0.63136034, 0.45626977,\n",
       "       0.32574865, 0.55825388, 0.6823347 , 0.69455446, 0.58428157,\n",
       "       0.52804149, 0.74062581, 0.52571643, 0.4939777 , 0.30684765,\n",
       "       0.71800757, 0.4406114 , 0.60723808, 0.24466976, 0.69623721,\n",
       "       0.20300907, 0.61396595, 0.37198346, 0.55310728, 0.49505363,\n",
       "       0.48061519, 0.44148876, 0.3460227 , 0.37803065, 0.17441403,\n",
       "       0.52340181, 0.3054142 , 0.66199414, 0.65674939, 0.46710289,\n",
       "       0.63006656, 0.7901539 , 0.39927368, 0.65258364, 0.37501727,\n",
       "       0.52242676, 0.28810827, 0.61468576, 0.46302677, 0.48941848,\n",
       "       0.57336184, 0.64239537, 0.64152057, 0.56359014, 0.36178125,\n",
       "       0.26795967, 0.33842933, 0.65478295, 0.606384  , 0.4332111 ,\n",
       "       0.70469104, 0.67329443, 0.43165079, 0.25997591, 0.70664107,\n",
       "       0.36153395, 0.75553639, 0.80606918, 0.6415329 , 0.77059413,\n",
       "       0.27239317, 0.54723982, 0.4951159 , 0.65200453, 0.41012195,\n",
       "       0.57120044, 0.51820184, 0.60919177, 0.43490463, 0.74675404,\n",
       "       0.39036236, 0.55860498, 0.27539882, 0.44367513, 0.33122434,\n",
       "       0.47485132, 0.37446783, 0.32630288, 0.58007225, 0.44740625,\n",
       "       0.27539237, 0.3831883 , 0.69951409, 0.41938492, 0.53481926,\n",
       "       0.55189397, 0.5343497 , 0.48076651, 0.3811439 , 0.80134795,\n",
       "       0.64360051, 0.3207464 , 0.50220158, 0.60239217, 0.37910236,\n",
       "       0.61433798, 0.32028722, 0.49839534, 0.17883173, 0.56983859,\n",
       "       0.40082034, 0.6779596 , 0.52427795, 0.46843377, 0.693467  ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "第五步：获取文本特征\n",
    "       开始训练word2vec_cnn1d模型\n",
    "       取值不存在负数,可以用CNN模型，唯一的区别就是上个单元格word2vec对x_train进行标准化处理，这里是归一化处理,\n",
    "\"\"\"\n",
    "word2ver_bin_cnn1d = \"review_word2vec_cnn1d.bin\"\n",
    "\n",
    "print('start_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))\n",
    "#word2vec文本特征模型会将每一个词语转化为n维向量，所以内存会不够\n",
    "def  get_features_by_word2vec_cnn_1d():\n",
    "    global  max_features\n",
    "    global word2ver_bin_cnn1d\n",
    "    x_train, x_test, y_train, y_test=load_all_files()\n",
    "\n",
    "    x_train=cleanText(x_train)\n",
    "    x_test=cleanText(x_test)\n",
    "\n",
    "    x=x_train+x_test\n",
    "    cores=multiprocessing.cpu_count() #获取当前计算机的cpu个数\n",
    "\n",
    "    if os.path.exists(word2ver_bin_cnn1d):\n",
    "        print (\"Find cache file %s\" % word2ver_bin_cnn1d)\n",
    "        model=gensim.models.Word2Vec.load(word2ver_bin_cnn1d)\n",
    "    else:\n",
    "        model=gensim.models.Word2Vec(size=max_features, window=10, min_count=1, iter=60, workers=cores)\n",
    "\n",
    "        model.build_vocab(x)\n",
    "\n",
    "        model.train(x, total_examples=model.corpus_count, epochs=model.iter)\n",
    "        model.save(word2ver_bin_cnn1d)\n",
    "\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    x_train= np.concatenate([buildWordVector(model,z, max_features) for z in x_train])\n",
    "    #x_train = scale(x_train) #这里标准化处理数据介于[-1,1],但是CNN模型是不能负数？\n",
    "    x_train = min_max_scaler.fit_transform(x_train)\n",
    "    x_test= np.concatenate([buildWordVector(model,z, max_features) for z in x_test])\n",
    "    #x_test = scale(x_test)\n",
    "    x_test = min_max_scaler.transform(x_test)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "print (\"get_features_by_word2vec_cnn_1d\")\n",
    "x_train, x_test, y_train, y_test=get_features_by_word2vec_cnn_1d()\n",
    "print('end_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))\n",
    "x_train[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_time: 2021-11-17 18:40:10\n",
      "get_features_by_doc2vec\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/train/pos/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/train/neg/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/test/pos/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/test/neg/\n",
      "Find cache file review_doc2vec.bin\n",
      "end_time: 2021-11-17 18:40:20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.44783841, 0.42543603, 0.5684536 , 0.58249367, 0.31805432,\n",
       "       0.69776262, 0.43147905, 0.49470703, 0.54687621, 0.28639542,\n",
       "       0.46503794, 0.46804945, 0.51905424, 0.69251359, 0.41303784,\n",
       "       0.48299472, 0.61325353, 0.55412321, 0.66355049, 0.47657671,\n",
       "       0.49486381, 0.53723719, 0.56501481, 0.6408539 , 0.49864768,\n",
       "       0.67933776, 0.88649434, 0.28501082, 0.37121625, 0.5406572 ,\n",
       "       0.48228556, 0.56659812, 0.65961875, 0.47979771, 0.32683184,\n",
       "       0.37223713, 0.44664571, 0.62299622, 0.4392753 , 0.47715828,\n",
       "       0.53552034, 0.49536691, 0.46754355, 0.3941373 , 0.33787561,\n",
       "       0.56577254, 0.49515422, 0.57381413, 0.33702676, 0.24513948,\n",
       "       0.51113941, 0.4694651 , 0.49018001, 0.27253831, 0.41425049,\n",
       "       0.44726507, 0.58939833, 0.59209769, 0.59959119, 0.49908834,\n",
       "       0.32583658, 0.42472888, 0.40495883, 0.48870094, 0.28329501,\n",
       "       0.4072202 , 0.47014851, 0.54588969, 0.41900231, 0.59159348,\n",
       "       0.38075823, 0.5860117 , 0.5763693 , 0.5809582 , 0.67620026,\n",
       "       0.52967527, 0.48301178, 0.31135657, 0.57011731, 0.59500489,\n",
       "       0.38478972, 0.67101065, 0.4378255 , 0.4013033 , 0.72435214,\n",
       "       0.41662392, 0.4135001 , 0.39137903, 0.44538236, 0.33550655,\n",
       "       0.3655928 , 0.25217031, 0.5726225 , 0.51949662, 0.38276887,\n",
       "       0.25211669, 0.73375713, 0.55953103, 0.66851285, 0.43024269,\n",
       "       0.467304  , 0.5619122 , 0.37203422, 0.70202128, 0.48257421,\n",
       "       0.58016412, 0.55502273, 0.56960812, 0.50513206, 0.50205166,\n",
       "       0.73554389, 0.51413481, 0.38907612, 0.47320189, 0.53437143,\n",
       "       0.63108986, 0.49455269, 0.44751308, 0.47780291, 0.41566849,\n",
       "       0.45398883, 0.74137106, 0.89498859, 0.22105442, 0.46040398,\n",
       "       0.3014694 , 0.53248139, 0.46179613, 0.30488448, 0.64816885,\n",
       "       0.69113564, 0.42823038, 0.48118577, 0.42951157, 0.5512687 ,\n",
       "       0.6955483 , 0.40119061, 0.46381731, 0.50575888, 0.59898814,\n",
       "       0.53609795, 0.46767415, 0.5092047 , 0.59803928, 0.67158171,\n",
       "       0.37015096, 0.50956435, 0.31735635, 0.78053573, 0.54448781,\n",
       "       0.34379733, 0.64388483, 0.6359037 , 0.4551001 , 0.6732806 ,\n",
       "       0.6292905 , 0.17669159, 0.39696818, 0.65161648, 0.52488121,\n",
       "       0.56273951, 0.46515171, 0.44191168, 0.43041245, 0.52690413,\n",
       "       0.3909764 , 0.46706992, 0.37001044, 0.42392711, 0.57094032,\n",
       "       0.49760802, 0.55302377, 0.62081043, 0.75220427, 0.67819247,\n",
       "       0.66808865, 0.84409391, 0.47449476, 0.35130732, 0.57842031,\n",
       "       0.42383654, 0.44202518, 0.59455684, 0.354337  , 0.66171864,\n",
       "       0.53965866, 0.44854056, 0.5173215 , 0.54021359, 0.53728881,\n",
       "       0.3653306 , 0.47015626, 0.5864648 , 0.52926936, 0.30752071,\n",
       "       0.75789847, 0.21972445, 0.46638951, 0.73461965, 0.36351309])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "第五步：获取文本特征\n",
    "       开始训练doc2vec模型\n",
    "\"\"\"\n",
    "doc2ver_bin = \"review_doc2vec.bin\"\n",
    "import datetime\n",
    "print('start_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))\n",
    "def  get_features_by_doc2vec():\n",
    "    global  max_features\n",
    "    x_train, x_test, y_train, y_test=load_all_files()\n",
    "\n",
    "    x_train=cleanText(x_train)\n",
    "    x_test=cleanText(x_test)\n",
    "    \n",
    "    #这里的x_train作为输入不仅包括review，还包括段落id\n",
    "    x_train = labelizeReviews(x_train, 'TRAIN')\n",
    "    x_test = labelizeReviews(x_test, 'TEST')\n",
    "\n",
    "    x=x_train+x_test\n",
    "    cores=multiprocessing.cpu_count()\n",
    "    #models = [\n",
    "        # PV-DBOW\n",
    "    #    Doc2Vec(dm=0, dbow_words=1, size=200, window=8, min_count=19, iter=10, workers=cores),\n",
    "        # PV-DM w/average\n",
    "    #    Doc2Vec(dm=1, dm_mean=1, size=200, window=8, min_count=19, iter=10, workers=cores),\n",
    "    #]\n",
    "    if os.path.exists(doc2ver_bin):\n",
    "        print (\"Find cache file %s\" % doc2ver_bin)\n",
    "        model=Doc2Vec.load(doc2ver_bin)\n",
    "    else:\n",
    "        model=Doc2Vec(dm=0, size=max_features, negative=5, hs=0, min_count=2, workers=cores,iter=60)\n",
    "        model.build_vocab(x)\n",
    "        model.train(x, total_examples=model.corpus_count, epochs=model.iter)\n",
    "        model.save(doc2ver_bin)\n",
    "    \n",
    "    x_test=getVecs(model,x_test,max_features)\n",
    "    x_train=getVecs(model,x_train,max_features)\n",
    "    min_max_scaler = preprocessing.MinMaxScaler() #这里我们自己进行归一化处理，看看数据有没有用\n",
    "    \n",
    "    x_train = min_max_scaler.fit_transform(x_train)\n",
    "    x_test = min_max_scaler.transform(x_test)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "print (\"get_features_by_doc2vec\")\n",
    "x_train, x_test, y_train, y_test=get_features_by_doc2vec()\n",
    "\n",
    "print('end_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))\n",
    "x_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_time: 2021-11-11 23:44:40\n",
      "start get_features_by_word2vec--->\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/train/pos/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/train/neg/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/test/pos/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/test/neg/\n",
      "Find cache file review_word2vec.bin\n",
      "start SVM\n",
      "0.87916\n",
      "[[11017  1483]\n",
      " [ 1538 10962]]\n",
      "start NB\n",
      "0.6714\n",
      "[[7829 4671]\n",
      " [3544 8956]]\n",
      "start RF\n",
      "0.74964\n",
      "[[10221  2279]\n",
      " [ 3980  8520]]\n",
      "start DNN\n",
      "MLPClassifier(alpha=1e-05, hidden_layer_sizes=(5, 2), random_state=1,\n",
      "              solver='lbfgs')\n",
      "0.87048\n",
      "[[10838  1662]\n",
      " [ 1576 10924]]\n",
      "end_time: 2021-11-11 23:49:43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3_5\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "第六步:模型训练：\n",
    "基于word2vec提取的文本特征1\n",
    "\"\"\"\n",
    "import datetime\n",
    "print('start_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "print (\"start get_features_by_word2vec--->\")\n",
    "x_train, x_test, y_train, y_test=get_features_by_word2vec()\n",
    "\n",
    "print(\"start SVM\")\n",
    "do_svm_doc2vec(x_train, x_test, y_train, y_test)\n",
    "\n",
    "print(\"start NB\")\n",
    "do_nb_doc2vec(x_train, x_test, y_train, y_test)\n",
    "\n",
    "print(\"start RF\")\n",
    "do_rf_doc2vec(x_train, x_test, y_train, y_test)\n",
    "\n",
    "print(\"start DNN\")\n",
    "do_dnn_doc2vec(x_train, x_test, y_train, y_test)\n",
    "\n",
    "print('end_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_time: 2021-11-12 16:13:22\n",
      "start get_features_by_word2vec_cnn_1d--->\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/train/pos/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/train/neg/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/test/pos/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/test/neg/\n",
      "Find cache file review_word2vec_cnn1d.bin\n",
      "start SVM\n",
      "0.87864\n",
      "[[10970  1530]\n",
      " [ 1504 10996]]\n",
      "start NB\n",
      "0.67616\n",
      "[[7748 4752]\n",
      " [3344 9156]]\n",
      "start RF\n",
      "0.74616\n",
      "[[10115  2385]\n",
      " [ 3961  8539]]\n",
      "start DNN\n",
      "MLPClassifier(alpha=1e-05, hidden_layer_sizes=(5, 2), random_state=1,\n",
      "              solver='lbfgs')\n",
      "0.5\n",
      "[[12500     0]\n",
      " [12500     0]]\n",
      "end_time: 2021-11-12 16:15:29\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "第六步:模型训练：\n",
    "基于word2vec_cnn_1d提取的文本特征1\n",
    "\"\"\"\n",
    "import datetime\n",
    "print('start_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "print (\"start get_features_by_word2vec_cnn_1d--->\")\n",
    "x_train, x_test, y_train, y_test=get_features_by_word2vec_cnn_1d()\n",
    "\n",
    "print(\"start SVM\")\n",
    "do_svm_doc2vec(x_train, x_test, y_train, y_test)\n",
    "\n",
    "print(\"start NB\")\n",
    "do_nb_doc2vec(x_train, x_test, y_train, y_test)\n",
    "\n",
    "print(\"start RF\")\n",
    "do_rf_doc2vec(x_train, x_test, y_train, y_test)\n",
    "\n",
    "print(\"start DNN\")\n",
    "do_dnn_doc2vec(x_train, x_test, y_train, y_test)\n",
    "\n",
    "print('end_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1249  | total loss: \u001b[1m\u001b[32m0.69319\u001b[0m\u001b[0m | time: 251.676s\n",
      "| Adam | epoch: 005 | loss: 0.69319 - acc: 0.4956 -- iter: 24900/25000\n",
      "Training Step: 1250  | total loss: \u001b[1m\u001b[32m0.69318\u001b[0m\u001b[0m | time: 264.915s\n",
      "| Adam | epoch: 005 | loss: 0.69318 - acc: 0.4991 | val_loss: 0.69315 - val_acc: 0.5000 -- iter: 25000/25000\n",
      "--\n",
      "end_time: 2021-11-12 16:41:42\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "第六步:模型训练：\n",
    "基于word2vec_cnn_1d提取的文本特征2\n",
    "\"\"\"\n",
    "import datetime\n",
    "print('start_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))\n",
    "print (\"start get_features_by_word2vec_cnn_1d--->\")\n",
    "x_train, x_test, y_train, y_test=get_features_by_word2vec_cnn_1d()\n",
    "\n",
    "print(\"start word2vec_CNN_1d\")\n",
    "do_cnn_doc2vec(x_train, x_test, y_train, y_test)\n",
    "\n",
    "print('end_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_time: 2021-11-13 14:28:31\n",
      "start get_features_by_doc2vec--->\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/train/pos/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/train/neg/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/test/pos/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/test/neg/\n",
      "Find cache file review_doc2vec.bin\n",
      "start SVM\n",
      "0.87628\n",
      "[[10714  1786]\n",
      " [ 1307 11193]]\n",
      "start NB\n",
      "0.83764\n",
      "[[10151  2349]\n",
      " [ 1710 10790]]\n",
      "start RF\n",
      "0.67592\n",
      "[[9420 3080]\n",
      " [5022 7478]]\n",
      "start DNN\n",
      "MLPClassifier(alpha=1e-05, hidden_layer_sizes=(5, 2), random_state=1,\n",
      "              solver='lbfgs')\n",
      "0.5\n",
      "[[12500     0]\n",
      " [12500     0]]\n",
      "end_time: 2021-11-13 14:33:26\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "第六步:模型训练：\n",
    "基于doc2vec提取的文本特征1\n",
    "\"\"\"\n",
    "import datetime\n",
    "print('start_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "print (\"start get_features_by_doc2vec--->\")\n",
    "x_train, x_test, y_train, y_test=get_features_by_doc2vec()\n",
    "\n",
    "print(\"start SVM\")\n",
    "do_svm_doc2vec(x_train, x_test, y_train, y_test)\n",
    "\n",
    "print(\"start NB\")\n",
    "do_nb_doc2vec(x_train, x_test, y_train, y_test)\n",
    "\n",
    "print(\"start RF\")\n",
    "do_rf_doc2vec(x_train, x_test, y_train, y_test)\n",
    "\n",
    "print(\"start DNN\")\n",
    "do_dnn_doc2vec(x_train, x_test, y_train, y_test)\n",
    "\n",
    "print('end_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1249  | total loss: \u001b[1m\u001b[32m0.69319\u001b[0m\u001b[0m | time: 258.424s\n",
      "| Adam | epoch: 005 | loss: 0.69319 - acc: 0.4976 -- iter: 24900/25000\n",
      "Training Step: 1250  | total loss: \u001b[1m\u001b[32m0.69322\u001b[0m\u001b[0m | time: 271.597s\n",
      "| Adam | epoch: 005 | loss: 0.69322 - acc: 0.4938 | val_loss: 0.69316 - val_acc: 0.5000 -- iter: 25000/25000\n",
      "--\n",
      "end_time: 2021-11-13 15:19:10\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "第六步:模型训练：\n",
    "基于doc2vec提取的文本特征2:cnn_1d\n",
    "\"\"\"\n",
    "import datetime\n",
    "print('start_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "print (\"start get_features_by_doc2vec--->\")\n",
    "x_train, x_test, y_train, y_test=get_features_by_doc2vec()\n",
    "\n",
    "print(\"start doc2vec_CNN\")\n",
    "do_cnn_doc2vec(x_train, x_test, y_train, y_test)\n",
    "\n",
    "print('end_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_time: 2021-11-17 18:41:34\n",
      "start get_features_by_doc2vec--->\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/train/pos/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/train/neg/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/test/pos/\n",
      "Load E:/pycharm_project/deep_learning_web_safe/review/data/test/neg/\n",
      "Find cache file review_doc2vec.bin\n",
      "start doc2vec_CNN_2D\n",
      "WARNING:tensorflow:From D:\\anaconda3_5\\lib\\site-packages\\tflearn\\initializations.py:110: calling UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From D:\\anaconda3_5\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py:507: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n",
      "WARNING:tensorflow:From D:\\anaconda3_5\\lib\\site-packages\\tflearn\\initializations.py:165: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From D:\\anaconda3_5\\lib\\site-packages\\tflearn\\layers\\core.py:247: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From D:\\anaconda3_5\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "---------------------------------\n",
      "Run id: review\n",
      "Log directory: /tmp/tflearn_logs/\n",
      "INFO:tensorflow:Summary name Accuracy/ (raw) is illegal; using Accuracy/__raw_ instead.\n",
      "---------------------------------\n",
      "Training samples: 50\n",
      "Validation samples: 50\n",
      "--\n",
      "Training Step: 1  | time: 5.042s\n",
      "| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 | val_loss: 0.71773 - val_acc: 0.5000 -- iter: 50/50\n",
      "--\n",
      "Training Step: 2  | total loss: \u001b[1m\u001b[32m0.63403\u001b[0m\u001b[0m | time: 4.205s\n",
      "| Adam | epoch: 002 | loss: 0.63403 - acc: 0.2880 | val_loss: 2.06672 - val_acc: 0.5000 -- iter: 50/50\n",
      "--\n",
      "Training Step: 3  | total loss: \u001b[1m\u001b[32m0.70957\u001b[0m\u001b[0m | time: 2.746s\n",
      "| Adam | epoch: 003 | loss: 0.70957 - acc: 0.4615 | val_loss: 0.75330 - val_acc: 0.5000 -- iter: 50/50\n",
      "--\n",
      "Training Step: 4  | total loss: \u001b[1m\u001b[32m1.71810\u001b[0m\u001b[0m | time: 2.802s\n",
      "| Adam | epoch: 004 | loss: 1.71810 - acc: 0.4904 | val_loss: 1.15211 - val_acc: 0.5000 -- iter: 50/50\n",
      "--\n",
      "Training Step: 5  | total loss: \u001b[1m\u001b[32m1.04985\u001b[0m\u001b[0m | time: 2.668s\n",
      "| Adam | epoch: 005 | loss: 1.04985 - acc: 0.4970 | val_loss: 0.73244 - val_acc: 0.5000 -- iter: 50/50\n",
      "--\n",
      "Training Step: 6  | total loss: \u001b[1m\u001b[32m1.10967\u001b[0m\u001b[0m | time: 2.644s\n",
      "| Adam | epoch: 006 | loss: 1.10967 - acc: 0.4989 | val_loss: 1.05966 - val_acc: 0.5000 -- iter: 50/50\n",
      "--\n",
      "Training Step: 7  | total loss: \u001b[1m\u001b[32m0.88749\u001b[0m\u001b[0m | time: 2.713s\n",
      "| Adam | epoch: 007 | loss: 0.88749 - acc: 0.4996 | val_loss: 1.10848 - val_acc: 0.5000 -- iter: 50/50\n",
      "--\n",
      "Training Step: 8  | total loss: \u001b[1m\u001b[32m0.97843\u001b[0m\u001b[0m | time: 2.665s\n",
      "| Adam | epoch: 008 | loss: 0.97843 - acc: 0.4998 | val_loss: 0.75399 - val_acc: 0.5000 -- iter: 50/50\n",
      "--\n",
      "Training Step: 9  | total loss: \u001b[1m\u001b[32m1.03002\u001b[0m\u001b[0m | time: 2.706s\n",
      "| Adam | epoch: 009 | loss: 1.03002 - acc: 0.4999 | val_loss: 0.85076 - val_acc: 0.5000 -- iter: 50/50\n",
      "--\n",
      "Training Step: 10  | total loss: \u001b[1m\u001b[32m0.89380\u001b[0m\u001b[0m | time: 2.835s\n",
      "| Adam | epoch: 010 | loss: 0.89380 - acc: 0.5000 | val_loss: 0.95924 - val_acc: 0.5000 -- iter: 50/50\n",
      "--\n",
      "Training Step: 11  | total loss: \u001b[1m\u001b[32m0.88710\u001b[0m\u001b[0m | time: 2.725s\n",
      "| Adam | epoch: 011 | loss: 0.88710 - acc: 0.5000 | val_loss: 0.77065 - val_acc: 0.5000 -- iter: 50/50\n",
      "--\n",
      "Training Step: 12  | total loss: \u001b[1m\u001b[32m0.92028\u001b[0m\u001b[0m | time: 2.715s\n",
      "| Adam | epoch: 012 | loss: 0.92028 - acc: 0.5000 | val_loss: 0.72398 - val_acc: 0.5000 -- iter: 50/50\n",
      "--\n",
      "Training Step: 13  | total loss: \u001b[1m\u001b[32m0.85819\u001b[0m\u001b[0m | time: 2.811s\n",
      "| Adam | epoch: 013 | loss: 0.85819 - acc: 0.5000 | val_loss: 0.86003 - val_acc: 0.5000 -- iter: 50/50\n",
      "--\n",
      "Training Step: 14  | total loss: \u001b[1m\u001b[32m0.80224\u001b[0m\u001b[0m | time: 2.762s\n",
      "| Adam | epoch: 014 | loss: 0.80224 - acc: 0.5000 | val_loss: 0.78593 - val_acc: 0.5000 -- iter: 50/50\n",
      "--\n",
      "Training Step: 15  | total loss: \u001b[1m\u001b[32m0.82154\u001b[0m\u001b[0m | time: 2.842s\n",
      "| Adam | epoch: 015 | loss: 0.82154 - acc: 0.5000 | val_loss: 0.69427 - val_acc: 0.5000 -- iter: 50/50\n",
      "--\n",
      "Training Step: 16  | total loss: \u001b[1m\u001b[32m0.81606\u001b[0m\u001b[0m | time: 2.825s\n",
      "| Adam | epoch: 016 | loss: 0.81606 - acc: 0.5000 | val_loss: 0.78839 - val_acc: 0.5000 -- iter: 50/50\n",
      "--\n",
      "Training Step: 17  | total loss: \u001b[1m\u001b[32m0.77564\u001b[0m\u001b[0m | time: 2.830s\n",
      "| Adam | epoch: 017 | loss: 0.77564 - acc: 0.4856 | val_loss: 0.79356 - val_acc: 0.5000 -- iter: 50/50\n",
      "--\n",
      "Training Step: 18  | total loss: \u001b[1m\u001b[32m0.78469\u001b[0m\u001b[0m | time: 2.824s\n",
      "| Adam | epoch: 018 | loss: 0.78469 - acc: 0.4906 | val_loss: 0.70495 - val_acc: 0.5000 -- iter: 50/50\n",
      "--\n",
      "Training Step: 19  | total loss: \u001b[1m\u001b[32m0.78887\u001b[0m\u001b[0m | time: 2.807s\n",
      "| Adam | epoch: 019 | loss: 0.78887 - acc: 0.4937 | val_loss: 0.72056 - val_acc: 0.5000 -- iter: 50/50\n",
      "--\n",
      "Training Step: 20  | total loss: \u001b[1m\u001b[32m0.77140\u001b[0m\u001b[0m | time: 2.800s\n",
      "| Adam | epoch: 020 | loss: 0.77140 - acc: 0.4829 | val_loss: 0.77825 - val_acc: 0.5000 -- iter: 50/50\n",
      "--\n",
      "end_time: 2021-11-17 18:42:45\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "第六步:模型训练：\n",
    "基于doc2vec提取的文本特征2:cnn_2d\n",
    "trainX.shape = (50, 200, 500, 1)\n",
    "这里得CNN_2d我们是进行了处理的,需要进一步对y进行维度变化，以及one_hot编码,(确保进行模型训练的数据格式正确)\n",
    "\"\"\"\n",
    "import datetime\n",
    "print('start_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "print (\"start get_features_by_doc2vec--->\")\n",
    "x_train, x_test, y_train, y_test=get_features_by_doc2vec()\n",
    "\n",
    "print(\"start doc2vec_CNN_2D\")\n",
    "do_cnn_doc2vec_2d(x_train, x_test, y_train, y_test)\n",
    "\n",
    "print('end_time: ' + datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
